Will Tobey
wtobey1@jhu.edu

I, maybe unwisely, started off this project by implementing cuckoo hashing. I probably should have started at a simpler implementation, but my first go at cuckoo hashing went pretty well so I stuck with it. My first implementation was very basic, and used 2 tables and two power hash functions. I used a load factor of 0.5 and resized when I reached it. I did not resize when I couldn't place an element, just rehashed. I also allowed the placement of an entry to run for a constant number of iterations.

That submission did pretty well, so I started looking at ways to improve it. I started small, and improved how I determined when to give up on placing an element and rehash. I found using alog(n) where a is a constant is a good choice, after seeing it suggested in papers. 

Next, I tried more than 2 hash functions. I built an implementation of hash function that was very flexible in that it could use any number of hash functions. I tested it using Jaybee (more on this later), and found it did not actually improve the insertion speed of my data structure, and it hurt the speeds of finding an element. This made some sense. For finding an element, I now have to check h places, where h is the number of hash functions. Furthermore, although it saved me some rehashes, it wasnt enough to make up for the increased complexity of the insertions, at least in my first findings. I moved away from the idea and tried to optimize with 2 hash functions.

The next thing I did was merge the tables into one. I thought it would be more efficient to do all my look ups in one array. The speed difference ended up being minimal, but it also gave me another idea that might improve the table. The idea was to not restrict the each hash function to just half of the table, but let each one use the entire bounds of the table. I figured that would reduce the number of collisions. My first tests with that performed a bit faster for insertions and so I ran with it. I ended up finding a paper on Cuckoo hashing that cooberated the viability of the idea.

The next change I made was suggested by a paper I read on simplifying cuckoo hashing (found here https://link.springer.com/content/pdf/10.1007%2Fs11786-009-0005-x.pdf). The idea is to reduce the number of hashes you perfom with a simple tweak to the hash functions. It fixes a problem from the previous suggestion I implemented that when Im inserting, I dont know if I got a position from the first or second hash functions since they no longer fall under disjoint boundraries. As a result, I need an extra call to my hashing functions so I can check if the position is equal to the position I would expect from the first hash function. I can then decide which function to hash my next key with. To avoid this extra hashing, if I have a key K that maps to h1 and h2 with my universal hashing functions, then I say that it maps to h1 and h1 ^ h2. This means that during the insertion of a key, I can calculate a position using h1, then for the duration of the placement of the entry I obtain the position I want by doing (last position) ^ h2, where last position is the position I found an element at, and h2 is the second hash function for the element I found there. This was an interesting tweak, but ultimatly not a huge improvement. 

From there, I again focused on improving the insertion operations and went back to using two disjoint hash functions which I thought were easier and more common. I came accross a number of papers (https://link.springer.com/content/pdf/10.1007%2F978-3-540-87744-8_51.pdf) that talked about using a queue or a stash to remove elements that were tricky to insert or caused cycles. This could improve the frequency of rehashing, and increase the load factor with just a small, constant sized stash. After implementing this with a small improvment on my insertions, but a regression in lookup functions like has, get and put. I found this hurt performance with spell, and moved on from it. I think that this idea is theoretically viable, but in practice, once I hit a single cycle I would hit many more in quick succession because I was close to my load factor. In the end it just prolonged my rehashing, and forced me to perform long insertions when I was at or beyond my rehash, when I could perform quicker insertions by just rehashing and resizing, which I more often than not would have to do anyway.

Here I hit a bit of a wall, but eventually found a new direction to go. I was inspired by this paper https://link.springer.com/content/pdf/10.1007%2Fs11786-009-0005-x.pdf which describes relativly simple ways to improve cuckoo hashing. The first one was to use tombstones to increase the speed of my search. When an item is removed, a tomb stone is placed. An additional rule is that you always place in the first hash position. This way, when the hash is asked to find an key, if it checks the first hash function and finds null (not an entry and not a tombstone), it can guarentee that the key is not in the array because if it was it would be at that position. This improved my finding function, which in turn improved my insertions, has, get and put. Another modification I made based on this paper was checking if I could place into the second hash position before evicting from the first. This means instead of automatically evicting, I would see if I could use the other hash function and possibly avoid longer insertions. This proved to speed up insertion noticably. This implementation gave me my best time yet.

Lastly I read about asymetric tables. This means that I have 1 table with 1 hash function that is 2 times larger than the other hash table. This allows more lookups to succeed after the first hash. An added benifit is that it allows insertions to succeed faster since the initial position has a better chance of being empty. I implemented this, but quickly realized I could get rid of the improvements I made earlier (the tombstones and the hashing check) because they didnt help me as much in this implementation. That is because with more insertions in the first table, I am more likely to find the element on the first hash, and so I do not save much time by using tombstones. Actually, I found tombstones ended up slowing down this implementation (and probably hurt memory). And for the insertion optimization I mentioned last paragraph, it is more likely I will be able to place in the first table with no collisions, so checking the second table first doesnt help as much. Instead it adds more hashes and memory accesses. 